{
    "version": "https://jsonfeed.org/version/1",
    "title": "超算之路",
    "description": "记录从0开始的超算学习之路",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2024/09/16/%E8%B6%85%E7%AE%97%E7%9C%9F%E9%A2%98%E4%B8%80%EF%BC%9AOpenCAEPoro%E7%9A%84%E4%BC%98%E5%8C%96/",
            "url": "http://example.com/2024/09/16/%E8%B6%85%E7%AE%97%E7%9C%9F%E9%A2%98%E4%B8%80%EF%BC%9AOpenCAEPoro%E7%9A%84%E4%BC%98%E5%8C%96/",
            "title": "超算真题一：OpenCAEPoro的优化",
            "date_published": "2024-09-16T14:37:35.000Z",
            "content_html": "<h1 id=\"赛题编译\"><a href=\"#赛题编译\" class=\"headerlink\" title=\"赛题编译\"></a>赛题编译</h1><p>在正式开始之前，我尝试配置cuda、git等环境，但都因权限受限没有成功。于是这里直接使用原有的<strong>cuda_12.2</strong>以及windows本地的<strong>git bash</strong>，整个克隆、编译及优化过程都是通过 <strong>MobaXterm</strong> 完成的，这里也非常推荐大家使用这款免费且功能强大的远程终端控制软件。这也是我在个人博客网站利用 <strong>markdown</strong> 写的第一篇博客，欢迎大家留言讨论！</p>\n<h2 id=\"赛题克隆\"><a href=\"#赛题克隆\" class=\"headerlink\" title=\"赛题克隆\"></a>赛题克隆</h2><p>首先我们进入项目主页：<a href=\"https://github.com/OpenCAEPlus/OpenCAEPoro_ASC2024\">OpenCAEPoro for ASC 2024 </a>，之后进行下图选择复制该项目的URL。</p>\n<p><img loading=\"lazy\" data-src=\"D:\\HexoBlog\\blog\\source\\imgs\\OpenCAEPoro\\项目url.png\"></p>\n<p>完成后，在本地新建一个文件夹，并在此文件夹内启动 <strong>git bash</strong> 输入<code>git init</code>初始化项目，之后执行如下命令进行项目的克隆（克隆其他项目时只需要更换URL即可）：</p>\n<pre><code class=\"bash\">git clone https://github.com/OpenCAEPlus/OpenCAEPoro_ASC2024.git\n</code></pre>\n<p>克隆成功后如下图所示（PS：末尾加的<code>--depth 1</code>是为了加速 clone ，此项目无需历史版本及更新，因此可进行此操作加快克隆）</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/%E5%85%8B%E9%9A%86%E6%88%90%E5%8A%9F.png\"></p>\n<h2 id=\"依赖安装及环境配置\"><a href=\"#依赖安装及环境配置\" class=\"headerlink\" title=\"依赖安装及环境配置\"></a>依赖安装及环境配置</h2><h3 id=\"创建目录\"><a href=\"#创建目录\" class=\"headerlink\" title=\"创建目录\"></a>创建目录</h3><p>首先我们在用户 home 目录下创建一个名为 <strong>OpenCAEPoro</strong> 的父文件夹，用于存储本项目的所有文件，并在此基础上下设两个子文件夹：<strong>libraries</strong> 用于存储依赖， <strong>src</strong> 用于存储源码（后续发现解压 OpenCAEPoro 后，其解压目录下自带 src 文件夹，故后续删除了父目录下的 src 文件夹。</p>\n<pre><code class=\"bash\">mkdir OpenCAEPoro &amp;&amp; cd OpenCAEPoro\nmkdir libraries src\n</code></pre>\n<p>之后通过 <strong>MobaXterm</strong> 将所需依赖移动到 <strong>libraries</strong> 下，OpenCAEPoro的安装包直接放在父文件夹下。然后来到 <strong>libraries</strong> 文件夹，依次解压依赖安装包以方便下面进行安装。 同时直接在 <strong>OpenCAEPoro</strong> 的父文件夹下解压OpenCAEPoro。</p>\n<pre><code class=\"bash\">tar zxvf hypre-2.28.0.tar.gz\ntar zxvf parmetis-4.0.3.tar.gz\ntar zxvf petsc_solver.tar.gz\ntar zxvf lapack-3.11.tar.gz\ntar zxvf petsc-3.19.3.tar.gz\ncd ..\ntar zxvf OpenCAEPoro.tar.gz \n</code></pre>\n<h3 id=\"环境配置\"><a href=\"#环境配置\" class=\"headerlink\" title=\"环境配置\"></a>环境配置</h3><h4 id=\"oneapi2024-0的环境配置\"><a href=\"#oneapi2024-0的环境配置\" class=\"headerlink\" title=\"oneapi2024.0的环境配置\"></a>oneapi2024.0的环境配置</h4><p><u>注：根据oneapi官网的<a href=\"https://www.intel.com/content/www/us/en/developer/articles/release-notes/oneapi-c-compiler-release-notes.html\">更新公告</a>, icc，icpc在2023.2月之后的版本被废弃，取而代之的是icx</u>。而服务器的MPI是2021版的，但是Compiler是2024版的,而MPI里面的程序用到了Compiler里的icc，于是这里不得不使用老版本的oneapi进行配置，请点此跳转：<a href=\"#%E2%91%A1oneapi2022.2%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE\">旧版本oneapi配置</a></p>\n<p>我的用户所在目录自带 <strong>cmake3.22.1</strong> ，所以这里主要进行 <strong>oneapi</strong> 的环境配置。其实这里不需要自己重新下载安装的，直接在服务器的根目录下找到 <strong>oneapi</strong> 的安装位置即可（绝大多数位置都是一样的）。</p>\n<pre><code class=\"bash\">cd /opt/intel/oneapi &amp;&amp; ls\n</code></pre>\n<p>执行上文指令后即可看到我们服务器配置了2024最新版的oneapi，其中的 <strong>setvars.sh</strong> 文件<strong>包含了所安装的所有oneAPI工具包的环境变量</strong>。确定好环境变量所在的位置，我们接下来进行配置即可。执行<code>vi ~/.bashrc</code>，并在文件末尾添加如下语句（将 <code>xxx</code> 改为你的用户名）</p>\n<pre><code class=\"bash\">source /opt/intel/oneapi/setvars.sh intel64\n</code></pre>\n<p>退出保存后再执行<code>source ~/.bashrc</code>使环境变量立即生效即可，之后执行如下指令进行环境测试。</p>\n<pre><code class=\"bash\">icx -v\ngdb-oneapi -v\n</code></pre>\n<p>如下图一样正常回显版本信息即为配置成功！</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20240921174711807.png\" alt=\"image-20240921174711807\"></p>\n<h4 id=\"oneapi2022-2的环境配置\"><a href=\"#oneapi2022-2的环境配置\" class=\"headerlink\" title=\"oneapi2022.2的环境配置\"></a>oneapi2022.2的环境配置</h4><p>首先执行以下命令进行安装包的下载及安装。</p>\n<pre><code class=\"bash\">wget https://registrationcenter-download.intel.com/akdlm/irc_nas/18679/l_HPCKit_p_2022.2.0.191_offline.sh\nsh l_HPCKit_p_2022.2.0.191_offline.sh\n</code></pre>\n<p>当然这里也可以直接先下载到本地，再直接传到服务器上进行安装。</p>\n<pre><code class=\"bash\">cd ~/intel/oneapi &amp;&amp; ls\n</code></pre>\n<p>执行以上指令后即可看到我们刚刚安装好的2022.2版本的oneapi，其中的 <strong>setvars.sh</strong> 文件<strong>包含了所安装的所有oneAPI工具包的环境变量</strong>。确定好环境变量所在的位置，我们接下来进行配置即可。执行<code>vi ~/.bashrc</code>，并在文件末尾添加如下语句（将 <code>xxx</code> 改为你的用户名）。</p>\n<pre><code class=\"bash\">source /home/xxx/intel/oneapi/setvars.sh intel64\n</code></pre>\n<p>退出保存后再执行<code>source ~/.bashrc</code>使环境变量立即生效即可，之后执行如下指令进行环境测试。</p>\n<pre><code class=\"bash\">icx -v\ngdb-oneapi -v\n</code></pre>\n<p>如下图一样正常回显版本信息即为配置成功！</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20240921204248392.png\" alt=\"image-20240921204248392\"></p>\n<h3 id=\"依赖安装\"><a href=\"#依赖安装\" class=\"headerlink\" title=\"依赖安装\"></a>依赖安装</h3><h4 id=\"lapack安装\"><a href=\"#lapack安装\" class=\"headerlink\" title=\"lapack安装\"></a>lapack安装</h4><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd lapack-3.11\nmake blaslib\nmake cblaslib\nmake lapacklib\nmake lapackelib\n</code></pre>\n<h4 id=\"parmetis安装\"><a href=\"#parmetis安装\" class=\"headerlink\" title=\"parmetis安装\"></a>parmetis安装</h4><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd parmetis-4.0.3\nvim build-parmetis.sh\n#进入编辑模式后，将下方代码进行修改并保存退出\nmake configcc=mpiicc prefix=ROOT_DIR/parmetis-4.0.3/parmetis-install #修改“build-parmetis.sh”脚本的安装路径\n#其中 ROOT_DIR 是存储库的根目录，可根据自己实际的目录设置（下同）\nsh build-parmetis.sh\n</code></pre>\n<h4 id=\"hypre安装\"><a href=\"#hypre安装\" class=\"headerlink\" title=\"hypre安装\"></a>hypre安装</h4><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd hypre-2.28.0\nvim build-hypre.sh\n#进入编辑模式后，将下方代码进行修改并保存退出\n./configure --prefix=ROOT_DIR/hypre-2.28.0/install --with-MPI --enable-shared\n###\nsh build-hypre.sh\n</code></pre>\n<h4 id=\"petsc安装\"><a href=\"#petsc安装\" class=\"headerlink\" title=\"petsc安装\"></a>petsc安装</h4><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd petsc-3.19.3\nvim build-petsc.sh \n#进入编辑模式后，将下方代码进行修改并保存退出\n###\nexport PETSC_DIR=ROOT_DIR/petsc-3.19.3\nexport PETSC_ARCH=petsc_install\n./configure CC=mpiicc CXX=mpiicpc \\\n--with-fortran-bindings=0 \\\n--with-hypre-dir=ROOT_DIR/hypre-2.28.0/install \\\n--with-debugging=0 \\\nCOPTFLAGS=&quot;-O3&quot; \\\nCXXOPTFLAGS=&quot;-O3&quot; \\\nmake -j 20 PETSC_DIR=ROOT_DIR/petsc-3.19.3 PETSC_ARCH=petsc_install all\n###\nsh build-petsc.sh\n</code></pre>\n<h4 id=\"petsc-solver安装\"><a href=\"#petsc-solver安装\" class=\"headerlink\" title=\"petsc_solver安装\"></a>petsc_solver安装</h4><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd petsc_solver\nvim build-petscsolver.sh\n#进入编辑模式后，将下方代码进行修改并保存退出\n###\nexport CPATH=ROOT_DIR/lapack-3.11/CBLAS/include:ROOT_DIR/lapack-3.11/LAPACKE/include:$CPATH\nexport LD_LIBRARY_PATH=ROOT_DIR/lapack-3.11:$LD_LIBRARY_PATH\n###\nvim CMakeLists.txt\n#进入编辑模式后，将下方代码进行修改并保存退出\n###\nset(PETSC_DIR &quot;ROOT_DIR/petsc-3.19.3/&quot;)\nset(PETSC_ARCH &quot;petsc_install&quot;)\n###\nsh build-petscsolver.sh\n</code></pre>\n<h2 id=\"OpenCAEPoro的安装与运行\"><a href=\"#OpenCAEPoro的安装与运行\" class=\"headerlink\" title=\"OpenCAEPoro的安装与运行\"></a>OpenCAEPoro的安装与运行</h2><h3 id=\"OpenCAEPoro的安装\"><a href=\"#OpenCAEPoro的安装\" class=\"headerlink\" title=\"OpenCAEPoro的安装\"></a>OpenCAEPoro的安装</h3><p>执行以下指令进入解压后的文件夹并安装。</p>\n<pre><code class=\"bash\">cd OpenCAEPoro\nvim mpi-build-petsc.sh\n#进入编辑模式后，将下方代码进行修改并保存退出\n###\nexport PARMETIS_DIR=ROOT_DIR/parmetis-4.0.3\nexport PARMETIS_BUILD_DIR=ROOT_DIR/parmetis-4.0.3/build/Linux-x86_64\nexport METIS_DIR=ROOT_DIR/parmetis-4.0.3/metis\nexport METIS_BUILD_DIR=ROOT_DIR/parmetis-4.0.3/build/Linux-x86_64\nexport PETSC_DIR=ROOT_DIR/petsc-3.19.3\nexport PETSC_ARCH=petsc_install\nexport PETSCSOLVER_DIR=ROOT_DIR/petsc_solver\nexport CPATH=ROOT_DIR/petsc-3.19.3/include/:$CPATH\nexport CPATH=ROOT_DIR/petsc-3.19.3/petsc_install/include/:ROOT_DIR/parmetis-4.0.3/metis/include:\nROOT_DIR/parmetis-4.0.3/include:$CPATH\nexport CPATH=ROOT_DIR/lapack-3.11/CBLAS/include/:$CPATH\n###\nsh mpi-build-petsc.sh\n</code></pre>\n<h3 id=\"OpenCAEPoro的运行\"><a href=\"#OpenCAEPoro的运行\" class=\"headerlink\" title=\"OpenCAEPoro的运行\"></a>OpenCAEPoro的运行</h3><p>继续在此目录下运行<code>mpirun -np &lt;core_num&gt; ./testOpenCAEPoro ./data/case1/case1.data verbose=1</code>进行运行测试（<core_num>为<strong>进程数</strong>，可自行指定，这里设为8）。程序的运行情况如下图所示。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20240926100711674.png\" alt=\"image-20240926100711674\"></p>\n<p>其中，<code>Timestep</code> 表示每一步的时间步长，<code>Wall time</code> 表示每一步仿真所耗费的实际计算时间。这里对三条警告信息进行解释：</p>\n<ul>\n<li><strong>NR not fully converged</strong>：这代表数值解法（Newton-Raphson方法）未完全收敛。尽管未收敛，但程序仍在继续运行，以趋于<strong>稳定</strong>。</li>\n<li><strong>Cut time step size</strong>：时间步长被缩小的提示，这通常是为了确保数值计算的稳定性。当出现非收敛情况时，程序会自动减小时间步以尝试解决<strong>不稳定</strong>或者<strong>不合理</strong>的解。</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20240926100754075.png\" alt=\"image-20240926100754075\"></p>\n<ul>\n<li><strong>Negative Ni</strong> 警告：仿真中的 <code>Ni</code> 值出现负值，表示仿真中的某些变量在某个位置出现了不合理的数值结果。</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003163802399.png\" alt=\"image-20241003163802399\"></p>\n<p>最终的运行结果如上图所示。</p>\n<h1 id=\"赛题优化\"><a href=\"#赛题优化\" class=\"headerlink\" title=\"赛题优化\"></a>赛题优化</h1><h2 id=\"优化思路\"><a href=\"#优化思路\" class=\"headerlink\" title=\"优化思路\"></a>优化思路</h2><p>对于HPC优化而言，无非就是两大类：①以优化<strong>并行度</strong>和<strong>进程数</strong>为核心的“<strong>硬件优化</strong>”，即“<strong>并行优化</strong>”②以优化<strong>算法</strong>、<strong>编译器</strong>、<strong>数学库</strong>为核心的“<strong>软件优化</strong>”。在没有明显算法瓶颈的情况下，通过增加进程数、节点数，可以迅速获得性能提升。如果性能瓶颈是因为算法的效率低下（比如存在大量冗余计算或非最优的数值方法），软件优化能带来更大提升。并且如果程序的并行扩展性不好，增加进程数反而导致性能下降（因为通信开销增加等问题），此时需要先优化算法。</p>\n<p>而从上图不难看出，在进行迭代时，Newton步数和线性步数都存在浪费，意味着某些<strong>迭代过程未能快速收敛</strong>，这部分耗时是潜在的优化点。分析模拟时间，可以发现<strong>线性求解器（Linear Solver）</strong>是性能瓶颈（不完全对，解释<a href=\"#%E7%BA%BF%E6%80%A7%E6%B1%82%E8%A7%A3%E5%99%A8\">点此跳转</a>，占用了91.611%的时间，主要的耗时来源。同时，<strong>Newton 步数</strong>占比非常小，表明 Newton 的执行效率较高。结合 <strong>Object Time</strong> 还可以得到<strong>Assembling</strong>是除线性求解器外的主要耗时来源。而<strong>通信</strong>花费了相对较少的时间，后续在<strong>并行环境</strong>下再进行测试。</p>\n<p>因此这里的主体优化采取“<strong>先软后硬</strong>”的优化策略。</p>\n<h2 id=\"进程修改\"><a href=\"#进程修改\" class=\"headerlink\" title=\"进程修改\"></a>进程修改</h2><p>因为pdf文件中告知，<strong>改变运行的进程数是最有效的方法</strong>，因此这里先寻找一个最佳进程数。经过多次尝试，博主发现当进程数为52时，程序的<code>object time</code>最小，为54.890s，如下图所示：</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003225420855.png\" alt=\"image-20241003225420855\"></p>\n<p>但是为什么随着进程数的增加，<code>object time</code>会先降后增呢？经过查阅与思考后，得到以下几条原因：</p>\n<ul>\n<li>在增加进程数时，如果任务不能均匀分配到所有进程中，程序完成不同步，<strong>负载不均衡</strong>。</li>\n<li>随着进程数增加，进程间<strong>通信开销</strong>也会增加，通信频繁的情况会导致性能瓶颈。</li>\n<li><strong>数据</strong>需要在更多进程之间<strong>传输</strong>，可能引入更多的<strong>延迟</strong>和开销。</li>\n<li>多个进程可能<strong>竞争同一资源</strong>（如 CPU、内存、I&#x2F;O 设备），当进程数过多时，资源争用加剧。</li>\n</ul>\n<p>因此，后续通过多机提升<strong>并行度</strong>时，也需考虑<strong>通信开销</strong>、<strong>负载均衡</strong>等问题，此内容将在硬件优化部分介绍，这里不再赘述。</p>\n<h2 id=\"软件优化\"><a href=\"#软件优化\" class=\"headerlink\" title=\"软件优化\"></a>软件优化</h2><h3 id=\"性能分析准备\"><a href=\"#性能分析准备\" class=\"headerlink\" title=\"性能分析准备\"></a>性能分析准备</h3><p>我们这里选择 <strong>Intel VTune Profiler</strong> 进行性能分析，因为他有非常直观的图形化界面，且支持详细的热点分析、并行性能评估（如负载均衡、线程同步问题）以及硬件性能计数器。首先我们在超算服务器上找到 <strong>vtune</strong> 的环境变量路径，之后在用户目录下执行以下代码进行环境配置。</p>\n<pre><code class=\"bash\">vim ~/.bashrc\n#进入编辑模式后，将下方代码写入底部\nsource /opt/intel/oneapi/vtune/2022.4.0/env/vars.sh #因为oneapi是2022的版本，出于兼容性的考虑，这里也选用2022版本的vtune\n#保存退出后，更新环境变量\nsource ~/.bashrc\n</code></pre>\n<p>之后执行<code>vtune-gui</code>进行测试以打开图形化界面，出现下图界面即代表配置成功。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003153354108.png\" alt=\"image-20241003153354108\"></p>\n<h3 id=\"线性求解器优化\"><a href=\"#线性求解器优化\" class=\"headerlink\" title=\"线性求解器优化\"></a>线性求解器优化</h3><p>这里本来打算利用vtune辅助定位进行优化的，可是vtune只能分析可执行二进制文件，而源文件<code>PETSC_FIM_solver.cpp</code>没有进行编译，在编译过程中，遇到了一些问题，比如<strong>头文件的嵌套调用</strong>、<strong>链接器未正确链接到所需的 PETSc 库</strong>等问题。解决一系列问题后，发现<code>petsc-config</code>命令更新为了<code>pkg-config</code>，后续又出现了一系列的兼容问题，为了不耽误更多时间，以下内容不再使用vtune进行定位优化，而是凭借程序设计经验对复杂度进行优化。</p>\n<p>为了操作快捷，这里先将该代码拷贝到本地修改，之后再传回服务器进行测试。后续修改的部分源代码如下：</p>\n<pre><code class=\"c++\">// Modifiable Area  v\n    for (int i = 0; i &lt; nBlockRows; i++)\n    &#123;\n        nDCount[i] = 0;\n        for (int j = rpt[i]; j &lt; rpt[i + 1]; j++)\n        &#123;\n            if (cpt[j] &gt;= Istart &amp;&amp; cpt[j] &lt;= Iend)\n            &#123;\n                nDCount[i]++;\n            &#125;\n        &#125;\n    &#125;\n\n    for (int i = 0; i &lt; nBlockRows; i++)\n    &#123;\n        nNDCount[i] = rpt[i + 1] - rpt[i] - nDCount[i];\n    &#125;\n\n    int *globalx = (int *)malloc(blockSize * sizeof(int));\n    int *globaly = (int *)malloc(blockSize * sizeof(int));\n    for (Ii = 0; Ii &lt; nBlockRows; Ii++)\n    &#123;\n        for (i = 0; i &lt; blockSize; i++)\n        &#123;\n            globalx[i] = (Ii + Istart) * blockSize + i;\n        &#125;\n\n        for (int i = rpt[Ii]; i &lt; rpt[Ii + 1]; i++)\n        &#123;\n\n            for (int j = 0; j &lt; blockSize; j++)\n            &#123;\n                globaly[j] = cpt[i] * blockSize + j;\n            &#125;\n            ierr = MatSetValues(A, blockSize, globalx, blockSize, globaly, valpt, INSERT_VALUES);\n            CHKERRQ(ierr);\n            valpt += blockSize * blockSize;\n        &#125;\n    &#125;\n// Modifiable Area  ^\n</code></pre>\n<p>不难看出，这段代码中多次用到了嵌套循环，使复杂度达到了$O(n^2)$ 甚至$O(n^3)$，因此优化循环内的代码可获得更大的速度提升。同时，用于计算当前块在全局行索引（<code>globalx</code>）和全局列索引（<code>globaly</code>）中位置的变量，<strong>反复申请释放内存</strong>会增大内存分配开销，因此可以将其改为<strong>静态内存</strong>。</p>\n<h4 id=\"减少内存分配开销\"><a href=\"#减少内存分配开销\" class=\"headerlink\" title=\"减少内存分配开销\"></a>减少内存分配开销</h4><p>原代码如下：</p>\n<pre><code class=\"c++\">    int *globalx = (int *)malloc(blockSize * sizeof(int));\n    int *globaly = (int *)malloc(blockSize * sizeof(int));\n/----------------------------------------------------------------/\n    free(globalx);\n    free(globaly);\n</code></pre>\n<p>最简单的操作应该是将 <code>globalx</code> 和 <code>globaly</code> 等数组的<strong>内存分配移到外部</strong>，并进行<strong>静态分配</strong>，以减少内存分配开销，但是这样做就超出了代码可修改范围。因此这里只能在所指定的<code>FIM_solver_p_cpr</code>函数内进行修改。在查阅文档的时候，发现PETSc自己实现了一个内存池的操作——<code>PetscMalloc1</code>，能够<strong>高效对齐分配的内存</strong>。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003202136993.png\" alt=\"image-20241003202136993\"></p>\n<p>因此修改后的代码如下：</p>\n<pre><code class=\"c++\">    static int *globalx = nullptr;  //int *globalx = (int *)malloc(blockSize * sizeof(int));\n    if(globalx == NULL)\n    &#123;\n        PetscMalloc1(blockSize * sizeof(int), &amp;globalx);  //使用 PetscMa1loc1 高效分配对齐的内存 \n    &#125;\n\n    static int *globaly = nullptr;  //int *globaly = (int *)malloc(blockSize * sizeof(int));\n    if(globaly == NULL)\n    &#123;\n        PetscMalloc1(blockSize * sizeof(int), &amp;globaly);  //使用 PetscMa1loc1 高效分配对齐的内存\n    &#125;\n</code></pre>\n<p>重跑项目，发现<code>object time</code>为 54.533，性能几乎没有提升 ，如下图。失败案例+1</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241004011805774.png\" alt=\"image-20241004011805774\"></p>\n<h4 id=\"嵌套循环的优化\"><a href=\"#嵌套循环的优化\" class=\"headerlink\" title=\"嵌套循环的优化\"></a>嵌套循环的优化</h4><p>经过研究后，发现复杂度通过正常方法是没法改变的，可能需要借助更高级的数学库以降低复杂度，这里只找到了两处可以略微提高速率的地方，并且不再单独进行测试，肯定不会有很大的改进，代码如下：</p>\n<pre><code class=\"c++\">    for (int i = 0; i &lt; nBlockRows; i++)\n    &#123;\n        nDCount[i] = 0;\n        for (int j = rpt[i]; j &lt; rpt[i + 1]; j++)\n        &#123;\n            if (cpt[j] &gt;= Istart &amp;&amp; cpt[j] &lt;= Iend)\n            &#123;\n                nDCount[i]++;\n            &#125;\n        &#125;\n        nNDCount[i] = rpt[i + 1] - rpt[i] - nDCount[i]; /*合并 nDCount 和 nNDCount 的计算，减少不必要的遍历*/\n    &#125;\n</code></pre>\n<pre><code class=\"c++\">    for (Ii = 0; Ii &lt; nBlockRows; Ii++)\n    &#123;\n        int baseGlobalX = (Ii + Istart) * blockSize;  // 计算baseGlobalX，避免在循环中重复计算\n        for (i = 0; i &lt; blockSize; i++)\n        &#123;\n            globalx[i] = baseGlobalX + i;\n        &#125;\n\n        for (int i = rpt[Ii]; i &lt; rpt[Ii + 1]; i++)\n        &#123;\n            int baseGlobalY = cpt[i] * blockSize;  // 计算baseGlobalY，避免在循环中重复计算\n            for (int j = 0; j &lt; blockSize; j++)\n            &#123;\n                globaly[j] = baseGlobalY + j;\n            &#125;\n\n            ierr = MatSetValues(A, blockSize, globalx, blockSize, globaly, valpt, INSERT_VALUES);\n            CHKERRQ(ierr);\n            valpt += blockSize * blockSize;\n        &#125;\n    &#125;\n</code></pre>\n<h4 id=\"后续补充\"><a href=\"#后续补充\" class=\"headerlink\" title=\"后续补充\"></a>后续补充<a name=\"线性求解器\"></a></h4><p>后面利用vtune进行性能分析时，发现该可修改部分所处的函数，一共才用时不到16s，这说明<strong>在模拟时间的显示中，线性求解器所用的时间虽然是最长的，但是实际上petsc_solver只是整个线性求解过程中的一个子调用，有其他函数或模块在调用这个求解器，并在其中花费了大量时间。</strong>这里给我带来的经验就是，不要想当然，要通过具体的分析工具来进行确定！并且，<strong>矩阵计算并行化</strong>也会得到很大的提升，详见<a href=\"#%E5%B9%B6%E8%A1%8C%E4%BC%98%E5%8C%96*\">此处</a></p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003235632007.png\" alt=\"image-20241003235632007\"></p>\n<h3 id=\"OpenCAEPoro源文件优化\"><a href=\"#OpenCAEPoro源文件优化\" class=\"headerlink\" title=\"OpenCAEPoro源文件优化\"></a>OpenCAEPoro源文件优化</h3><h4 id=\"定位优化函数\"><a href=\"#定位优化函数\" class=\"headerlink\" title=\"定位优化函数\"></a>定位优化函数</h4><p>执行下方指令，利用vtune进行性能分析。</p>\n<pre><code class=\"bash\"> mpirun -np 52 vtune -collect hotspots -r ./vtune_results -- ./testOpenCAEPoro ./data/case1/case1.data verbose=1\n</code></pre>\n<p>分析结果如下图所示，<strong>位于前列的函数即为我们需要着重进行优化的函数</strong>。首先，<code>hypre_BoomerAMGBuildCoarseOperatorKT</code> 和 <code>hypre_BigBinarySearch</code>来自 HYPRE 库，虽然耗时很长，但由于它们属于外部库（<code>libHYPRE</code>），无法直接对其进行源码级别的优化。<code>PMPI_Waitall</code> 和 <code>PMPI_Testall</code>属于 MPI 库，表明程序的大量时间花费在等待和通信中。<strong>通信开销较高意味着并行化效率不足</strong>，优化详见<a href=\"#%E5%B9%B6%E8%A1%8C%E4%BC%98%E5%8C%96\">此处</a>。<code>MatSolve_SeqBAIJ_4_NaturalOrdering</code>和<code>MatMult_SeqBAIJ_4</code> 来自 PETSc 库，负责矩阵乘法和矩阵求解操作，后续可<strong>利用 GPU 优化矩阵操作</strong>。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241003235753565.png\" alt=\"image-20241003235753565\"></p>\n<h4 id=\"组分连接函数优化\"><a href=\"#组分连接函数优化\" class=\"headerlink\" title=\"组分连接函数优化\"></a>组分连接函数优化</h4><p>在<code>Partition</code>的源码中不难发现，其不同组分直接的连接是通过<strong>遍历数组</strong>实现的。这就导致查找不同组分的连接，在2x2x2的组分中，获得连接信息就要跑24次。所以这里尝试用哈希表降维打击。修改代码如下：</p>\n<pre><code class=\"c++\">/// Get neighbors&#39; process from other process\n    std::unordered_map&lt;idx_t, idx_t&gt; ump_right_neighbor; \n    std::unordered_map&lt;idx_t, idx_t&gt; ump_send_buffer; //创建哈希表\n    vector&lt;vector&lt;idx_t&gt;&gt;     left_neighbor_proc;\n    vector&lt;vector&lt;NeighborP&gt;&gt; right_neighbor;\n    vector&lt;vector&lt;idx_t&gt;&gt;     right_neighbor_proc;\n\n    for (idx_t i = 0; i &lt; numElementLocal; i++) &#123;\n        for (idx_t j = xadj[i]; j &lt; xadj[i + 1]; j++) &#123;\n            if (adjncy[j] &lt; vtxdist[myrank] || adjncy[j] &gt;= vtxdist[myrank + 1]) &#123;\n                // adjncy[j] 不在当前进程中，找到其对应的进程\n                // 首先进行估算\n                idx_t p = adjncy[j] / numElementLocal;\n                if (adjncy[j] &gt;= vtxdist[p]) &#123;\n                    while (adjncy[j] &gt;= vtxdist[p + 1]) &#123; p++; &#125;\n                &#125; else &#123;\n                    while (adjncy[j] &lt; vtxdist[--p]) &#123;&#125;\n                &#125;\n                // adjncy[j] 现在在进程 p 中 (p != myrank)\n                \n                if (ump_right_neighbor.find(p) != ump_right_neighbor.end()) &#123;\n                    // 如果进程 p 已经存在于right_neighbor中，直接访问\n                    idx_t k = ump_right_neighbor[p];\n                    right_neighbor[k].push_back(NeighborP(adjncy[j], j));\n                    left_neighbor_proc[k].push_back(part[i]);\n                &#125; else &#123;\n                    // 如果是新邻居进程\n                    right_neighbor.push_back(vector&lt;NeighborP&gt;&#123;NeighborP(p, p), NeighborP(adjncy[j], j)&#125;);\n                    left_neighbor_proc.push_back(vector&lt;idx_t&gt;&#123;p, part[i]&#125;);\n                    ump_right_neighbor[p] = right_neighbor.size() - 1;  // 更新哈希表\n                &#125;\n            &#125; else &#123;\n                // adjncy[j] 在当前进程\n                adjproc[j] = part[adjncy[j] - vtxdist[myrank]];\n            &#125;\n        &#125;\n    &#125;\n\n    \n    // Calculate necessary memory first\n    for (idx_t i = 0; i &lt; numElementLocal; i++) &#123;\n        if (part[i] == myrank) &#123;\n            recv_buffer[0][1]++;\n            recv_buffer[0][2] += (xadj[i + 1] - xadj[i]);\n        &#125;\n        else &#123;\n            // 使用哈希表进行查找\n            if (ump_send_buffer.find(part[i]) != ump_send_buffer.end()) &#123;\n                // 如果哈希表中已经有part[i]，直接访问\n                idx_t k = ump_send_buffer[part[i]] - 1;\n                send_buffer[k][1]++;\n                send_buffer[k][2] += (xadj[i + 1] - xadj[i]);\n            &#125;\n            else &#123;\n                // 如果哈希表中没有part[i]，新建条目\n                send_buffer.push_back(vector&lt;idx_t&gt;&#123;part[i], 1, xadj[i + 1] - xadj[i]&#125;);\n                ump_send_buffer[part[i]] = send_buffer.size();  // 更新哈希表，索引从1开始\n            &#125;\n        &#125;\n    &#125;\n</code></pre>\n<p>最终优化结果也是出乎意料的好，<code>object time</code>为44.833，性能提高了36.8%！不难看出<code>Assembling</code>所耗时间明显降低（从25.837秒到16.932秒），证明此优化极大地提高了连接不同组分的效率。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241004035307412.png\" alt=\"image-20241004035307412\"></p>\n<h2 id=\"硬件优化\"><a href=\"#硬件优化\" class=\"headerlink\" title=\"硬件优化\"></a>硬件优化</h2><h3 id=\"多机运行\"><a href=\"#多机运行\" class=\"headerlink\" title=\"多机运行\"></a>多机运行</h3><p>首先配置SSH密钥对以实现多机免密登录：</p>\n<pre><code class=\"bash\">mkdir .ssh\nssh-keygen -t rsa #连续输入三次回车即可\nssh-copy-id zhaozongxing@gpu02\nssh-copy-id zhaozongxing@gpu03\nssh-copy-id zhaozongxing@gpu04\n</code></pre>\n<p>之后编写hostfile文件，指定运行的机器以及进程数：</p>\n<pre><code class=\"txt\">gpu01:52\ngpu02:52\ngpu03:52\ngpu04:52\n</code></pre>\n<p>执行以下命令开始运行程序：</p>\n<pre><code class=\"bash\">mpirun -np 208 -machinefile ../hostfile ./testOpenCAEPoro ./data/case1/case1.data verbose=1\n</code></pre>\n<p>运行结果如下图，<code>object time</code>直接突破20秒，来到了<strong>19.719秒</strong>，相对于单机性能整整提高了一倍多。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241004053135367.png\" alt=\"image-20241004053135367\"></p>\n<h3 id=\"并行优化\"><a href=\"#并行优化\" class=\"headerlink\" title=\"并行优化*\"></a>并行优化*</h3><h4 id=\"矩阵计算并行化\"><a href=\"#矩阵计算并行化\" class=\"headerlink\" title=\"矩阵计算并行化\"></a>矩阵计算并行化</h4><p>其实这里不仅局限于矩阵计算，<strong>所有的循环基本都可以通过并行计算进行优化</strong>，出于时间问题，这里还是以<code>PETSC_FIM_solver</code>函数为例。当使用 <code>#pragma omp parallel for</code> 时，OpenMP 会将 <code>for</code> 循环的迭代任务划分给不同的线程，并行执行，从而加快代码的运行速度，在计算密集型任务中尤其明显。</p>\n<pre><code class=\"c++\">    #pragma omp parallel for\n    for (int i = 0; i &lt; nBlockRows; i++) &#123;\n        int local_nDCount = 0;\t/*这里使用新变量（local_nDCount）而不是直接使用nDCount[0]，\n                                是为了避免数据竞争，避免多个线程同时尝试写入同一个变量（nDCount[0]）*/\n        for (int j = rpt[i]; j &lt; rpt[i + 1]; j++) &#123;\n            if (cpt[j] &gt;= Istart &amp;&amp; cpt[j] &lt;= Iend) &#123;\n                local_nDCount++;\n            &#125;\n        &#125;\n        nDCount[i] = local_nDCount;\n        nNDCount[i] = rpt[i + 1] - rpt[i] - local_nDCount; \n    &#125;\n</code></pre>\n<pre><code class=\"c++\">    int localError = 0;\n\n    #pragma omp parallel for private(globalx, globaly, valpt) shared(localError) schedule(static)\n    /*\n    （1）每个线程在处理不同的 Ii 时会有不同的值，因此globalx, globaly, valpt等变量应该是私有的\n    （2）需要检查所有线程是否发生错误并在外部处理，因此应该共享localError变量\n    （3）schedule 指令可以优化线程负载分配和负载平衡，以提高并行效率。\n    */\n    for (Ii = 0; Ii &lt; nBlockRows; Ii++)\n    &#123;\n        for (i = 0; i &lt; blockSize; i++)\n        &#123;\n            globalx[i] = (Ii + Istart) * blockSize + i;\n        &#125;\n\n        for (int i = rpt[Ii]; i &lt; rpt[Ii + 1]; i++)\n        &#123;\n\n            for (int j = 0; j &lt; blockSize; j++)\n            &#123;\n                globaly[j] = cpt[i] * blockSize + j;\n            &#125;\n            ierr = MatSetValues(A, blockSize, globalx, blockSize, globaly, valpt, INSERT_VALUES);\n            if (ierr)\n            &#123;\n                #pragma omp critical // 防止多个线程同时修改 localError\n                &#123;localError = ierr;&#125;        \n            &#125;\n            valpt += blockSize * blockSize;\n        &#125;\n    &#125;\n    if (localError) &#123;\n        CHKERRQ(localError);//CHKERRQ(ierr)含return操作（当发生错误时会返回非零值），而OpenMP并行块中并不允许。\n    &#125;\n</code></pre>\n<p>单机运行时，<code>object time</code> 为<strong>42.162</strong>，多机运行时，<code>object time</code> 为<strong>18.236</strong>，相比之下，多机运行提升的效率更高。</p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241004064635777.png\" alt=\"image-20241004064635777\"></p>\n<p><img loading=\"lazy\" data-src=\"https://cdn.jsdelivr.net/gh/barbedcotton/BlogImage@main/img/image-20241004064846984.png\" alt=\"image-20241004064846984\"></p>\n<h4 id=\"其他并行化操作\"><a href=\"#其他并行化操作\" class=\"headerlink\" title=\"其他并行化操作\"></a>其他并行化操作</h4><p>除了矩阵运算外，还可以通过<strong>合并信息</strong>、<strong>非阻塞通信</strong>等手段来<strong>减少通信开销</strong>，而针对于<code>MPI</code>也可利用其<strong>集体通信函数</strong>，并通过调优其<strong>参数</strong>进行优化。因为时间问题，这里不再赘述，后续有时间可以对此进行更新！</p>\n",
            "tags": []
        }
    ]
}